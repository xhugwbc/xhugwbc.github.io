\section{Related Work}
\subsection{Cross-Embodiment Learning for Legged Robots}
Cross-embodiment learning aims to find control policies that generalize across robots with different morphologies and dynamics, reducing the need for embodiment-specific controllers.
% Early work in this area primarily focused on physics-based motion control and physical animation~\cite{huang2020smp, gupta2022metamorph, Trabucco2022AnyMorph}, as well as quadrupedal~\cite{feng2022genloco, Luo2024MorAL} and bipedal locomotion~\cite{yang2025multiloco, liulocoformer, ai2025towards}.
Most existing approaches train on a small set of embodiments and achieve limited transfer~\cite{yang2025multiloco, lin2025hzero, Doshi24-crossformer}, or introduce diversity through simple morphological randomization~\cite{weinan2022cross, Luo2024MorAL, liulocoformer, ai2025towards}. These methods typically assume aligned state-action spaces and similar embodiment dynamics, restricting their applicability to narrower robot families.
% In contrast, humanoids vary widely in kinematics, degrees of freedom, state representations, and dynamic properties. As a result, naive morphology randomization fails to capture this diversity and can reduce data efficiency. To mitigate state-space discrepancies, some methods map policy outputs to leg end-effector trajectories~\cite{shafiee2024manyquadrupeds}, followed by kinematic controllers. However, kinematics-level abstractions are inadequate for the tightly coupled whole-body coordination required in humanoid locomotion. Other approaches adopt joint-wise encodings to normalize representations across embodiments~\cite{bohlinger2024onepolicy, ai2025towards}, but still rely on limited embodiment diversity and struggle to scale to structurally distinct humanoid platforms.

% In contrast to these works, \our generates a diverse set of physically consistent humanoid embodiments through an optimization-based morphological randomization procedure and employs a hybrid-mask Transformer architecture to learn broad humanoid motion priors. This design enables robust cross-embodiment generalization with a single generalist policy, even across humanoid platforms with substantially different structures and dynamics.

Humanoid robots, however, exhibit heterogeneous structures, physics and state representations, thus violating such assumptions.
% Other efforts include biology-inspired trajectory generators~\cite{shafiee2024manyquadrupeds}, and joint-wise embodiment descriptors~\cite{bohlinger2024onepolicy, ai2025towards}.
To address this, some efforts resort to biology-inspired trajectory generators~\cite{shafiee2024manyquadrupeds}, while others represent embodiments with joint-level descriptor sequences~\cite{bohlinger2024onepolicy, ai2025towards}.
In contrast, \our generates physically consistent embodiment data and leverages a hybrid-mask Transformer to capture broad humanoid motion priors, enabling robust generalization with a single policy.

% This paradigm of cross-embodiment learning for legged robots is motivated by the need to develop generalizable control policies that can be rapidly adapted to robots with diverse morphologies, dynamics, and joint configurations. Such an approach seeks to transcend morphology-specific designs and enable robust transfer across heterogeneous legged platforms. Early work in cross-embodiment learning primarily targeted  physical-based motion control ~\cite{huang2020smp, gupta2022metamorph, Trabucco2022AnyMorph}, quadrupedal ~\cite{Luo2024MorAL, feng2022genloco} or bipedal ~\cite{yang2025multiloco, liulocoformer, ai2025towards} locomotion. The cross-embodiment policies are typically trained on a limited set of embodiments and achieve only modest transfer~\cite{yang2025multiloco, lin2025hzero, Doshi24-crossformer}, or rely on simple morphological randomization to inject diversity into the rollout data~\cite{weinan2022cross, Luo2024MorAL, liulocoformer, ai2025towards}. However, such approaches inherently assume invariant state spaces and equivalent dynamics across embodiments, restricting their applicability to narrowly defined domains. In contrast, humanoid robots exhibit highly variable state spaces that differ substantially across implementations, along with rich structural heterogeneity. As a result, na√Øve morphology randomization becomes ineffective and further exacerbate data scarcity. To address embodiment-specific variability, some studies attempted to mitigate state-space discrepancies by mapping policy outputs to leg end-effector trajectories ~\cite{shafiee2024manyquadrupeds}, which were then converted into joint commands through central pattern generators, but kinematics-level control proves insufficient for the full-body coordination required in humanoid locomotion. Other approaches seek to bridge the gap by adopting joint-wise encodings that normalize representations across embodiments ~\cite{bohlinger2024onepolicy, ai2025towards}. To compensate for data scarcity, some efforts have attempted to directly transfer randomization techniques developed for quadrupedal robots to humanoid platforms. However, such efforts have exhibited very limited generalization.
% \our insted generates physically consistent embodiment data through an optimization-based procedure and employs a hybrid-mask Transformer to capture broad humanoid motion priors, enabling robust cross-embodiment generalization of a single generalist policy.

\subsection{Humanoid Whole-Body Control}
% Whole-body control is fundamental for humanoid robots operating in real-world environments, yet remains challenging due to high-dimensional, contact-rich, and strongly coupled dynamics. 
Whole-body control is a critical yet challenging task for humanoid robot learning and applications in reality.
Recent work has explored unified command spaces~\cite{xue2025hugwbc, li2025amo, sun2025ulc, zhang2025unleashing}, typically using command sampling and reinforcement learning. Other studies focus on motion tracking and representation learning from individual demonstrations~\cite{xie2025kungfubot, he2025asap, huang2025adaptive} or large-scale datasets~\cite{yin2025unitracker, zhang2025any2track, ze2025twist2, luo2025sonic}, but often assume fixed robot morphologies. Additional progress has been made in whole-body loco-manipulation within controlled environments~\cite{wang2025physhsi, weng2025hdmi, su2025hitter, chen2025rhino, zhuang2025embracecollisions}.

Our work follows \citet{xue2025hugwbc} and adopts a unified command space to enable expressive whole-body behaviors while explicitly targeting cross-humanoid generalization.

