\section{Experiments}
We conduct comprehensive evaluations of \our in both simulation and the real-world environments, guided by the following research questions:

\begin{enumerate}[label=\textbf{RQ\arabic*.}, leftmargin=*]
\item How well does \our generalize to previously unseen embodiments?
\item How well can the generalist policy serve as an initialization for fine-tuning on each robot?
\item How does the proposed framework compare with cross-embodiment baselines?
\item Which policy architecture is more effective for building a generalist humanoid controller?
\end{enumerate}

\subsection{Evaluation on Unseen Robots}
For RQ1, we evaluate how the generalist policy learned by \our performs on unseen embodiments.
Table~\ref{tab:single_commands_baseline} summarizes results across 12 robots excluded from training, compared with specialist policies trained on each robot.
Appendix~\ref{tab:single commands error} provides detailed per-robot results.

The generalist policy demonstrates strong zero-shot generalization across diverse humanoid robots with substantial variations in kinematics, dynamics, and morphology. All robots evaluated achieve a 100\% survival rate and maintain consistently high command-tracking accuracy, without exhibiting bias toward any specific system.
While specialist policies remain an upper bound for each individual robot, \our achieves comparable performances across embodiments, with expected trade-offs due to lack of system-specific priors. Nevertheless, the generalist provides a powerful pretrained initialization, as discussed in Section~\ref{subsec:pretrain}.
% The few cases with larger tracking deviations primarily occur on smaller robots. This is largely due to use of a unified command range across all embodiments during evaluation, while their more limited actuation capabilities and morphological constraints restrict the achievable motion amplitudes.

\begin{figure}
    \centering
    \includegraphics[width=0.95 \linewidth]{images/tsne.pdf}
    \caption{\small \textbf{t-SNE visualization of transformer latent representation.} ``-DoF'' denotes the number of waist joints DoFs. The arrow ($\gets$) indicates the direction of increasing robot mass.}
    \label{fig:tsne}
    \vspace{-20pt}
\end{figure}

\paragraph{Qualitative Analysis.}
To gain insight into how \our works, we visualize the latent representations produced by the trained transformer before detokenization. Figure~\ref{fig:tsne} shows the t-SNE visualization of the latent vectors.
% In order to visualization the capabilities of \our in implicitly identifying specific robot types, we conducted 1,000 zero-shot trials across 12 humanoid robots, in which the commands were randomly sampled. Fig. \ref{fig:tsne} presents a t-distributed Stochastic Neighbor Embedding (t-SNE) analysis of mean output of the final Transformer layer. 
Here, we denote the ordering of the humanoid hip joints, such as $\mathbf{R}$oll, $\textbf{P}$itch, and $\textbf{Y}$aw, as $\textbf{R-P-Y}$, with other configurations defined similarly.
The results indicates that the structure of robot shapes the embedded latent distribution: robots with similar hip-joint arrangements cluster closely together.


The number of waist DoFs further modulates the embedding. For example, among robots with the $\textbf{R-P-Y}$ hip joint configuration, the robot in \textcolor[HTML]{BCBD22}{yellow} (with text \textbf{0-DoF}) has no actuated waist joints and appears near the center of the upper-right ring-shaped cluster. Robots  with a single waist Dofs (\textcolor[HTML]{FF7F00}{orange}, with text \textbf{1-DoF}) form an outer ring, with lighter robots positioned slightly inward (shown in $\star$). In contrast, robots with three waist DoFs (\textcolor[HTML]{D62728}{red} and \textcolor[HTML]{9467BD}{purple}, with text \textbf{3-DoF}) lie along the outermost boundary. 


\subsection{Fine-tuning the Generalist Policy}
\label{subsec:pretrain}
To investigate RQ2, we conduct per-robot fine-tuning experiments by reusing the learned weights.
Figure~\ref{fig:finetune_scratch_generalist} compares the training curves of i) generalist policy (\our), ii) embodiment-specific specialist policies (\cite{xue2025hugwbc}), and iii) the fine-tuned generalist policies (Generalist-FT) across 12 embodiments.
At convergence, the generalist policy reaches approximately 85\% of the return achieved by the specialist.
% We can also notice that the generalist policy exhibits a performance drop during the training process. This degradation is attributed to the curriculum learning schedule, which gradually increases task difficulty and temporarily reduces the observed returns.
A temporary drop in performance is observed during training, which arises from the curriculum schedule that progressively increases task difficulty.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/baseline_success_histogram.pdf}
    % \vspace{-6pt}
    \caption{\small \textbf{Zero-shot survival rate comparison across multiple baselines}. All policies are trained under the same training protocol. Naive Random is trained on data generated by the naive morphological randomization method, whereas \our, MetaMorph, and MorAL are trained using randomization in Sec.~\ref{sec: interpretable morphological randomization}.}
    \label{fig:baseline_success_histogram}
    \vspace{-20pt}
\end{figure}

On the other hand, fine-tuning the generalist policy yields significantly higher sample efficiency than training from scratch, and converges substantially faster than the other approaches, and the generalist rollouts show an approximate 10\% improvement in return compared to specialist ones. These results indicate that the learned representations of the generalist are highly adaptable, serving as a strong initialization for efficient specialization while retaining the potential for high-performance control across diverse robots.
Appendix~\ref{tab:single commands error} reports the tracking errors for all three policies, showing consistent findings.


\subsection{Comparing Cross-Embodiment Baselines}
For RQ3, Figure~\ref{fig:baseline_success_histogram} also compares \our with two \textit{cross-embodiment training} baselines, \textbf{MetaMorph}~\cite{gupta2022metamorph} and \textbf{MorAL}~\cite{Luo2024MorAL} (with the proposed morphology randomization); along with \textit{naive morphology randomization}~\cite{feng2022genloco, ai2025towards}(\textbf{Naive Random} in short) that approximates the rigid body of the robot as a simple rectangular, on zero-shot embodiment generalization. 
MetaMorph's policy relies on explicitly provided morphological information as input, including link inertial properties and joint motion ranges, and it is hard to learn generalizable representations from the high-dimensional, heterogeneous morphological descriptors of humanoid robots. 
MorAL performs even worse, with a substantial drop in performance observed on most robots, due to the limitations of the simple policy architectures and proprioceptive inputs without informative morphological features.
In addition, Naive Random generalizes only to a small subset of embodiments with similar mass distributions and kinematic features, while exhibiting significant performance degradation on the remaining robots.

Figure~\ref{fig:finetune_scratch_generalist} further reports the fine-tuned results for MetaMorph and MorAL. While fine-tuning two baselines benefit form improved initialization, their convergence is slower than Generalist-FT. In addition, their peak performance is comparable to that of specialist policies, but consistently lower than that achieved by the fine-tuned generalist.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/network_ablation_curve_two.pdf}
    \caption{\small \textbf{Network architecture ablation.} Training curves comparing MLP-, GNN-, and Transformer-based policies, reporting mean episodic return and mean episode length.}
    \label{fig:network_ablation_curve}
    \vspace{-20pt}
\end{figure}

\subsection{Network Ablation for the Generalist Controller}
To answer RQ4, we systematically evaluate different policy network architectures on training convergence and generalization to unseen robots. Figure~\ref{fig:network_ablation_curve} presents the training curves for MLP-, GNN-, and Transformer-based policies, where \textbf{(a)} reports mean episodic return and \textbf{(b)} reports mean episode length. Both the GNN and Transformer architectures significantly outperform the MLP baseline. Their performance gain stems from the ability to exploit the robot's embodied kinematic topology and dependency structure encoded in the embodiment adjacency matrix, leading to more sample-efficient learning.

In contrast, even when provided with explicit morphological information~\cite{Luo2024MorAL}, the MLP struggles to capture inter-joint dependencies. This is because a flattened state discards most of the underlying kinematic structure, making it difficult for the policy to infer meaningful relationships among joints. Based on these results, all subsequent experiments adopt the Transformer architecture.

\subsection{Real World Experiment}
We evaluated \our on 7 distinct humanoid robot platforms in real-world environments to test its zero-shot generalization and whole-body control capabilities, as shwon in Fig.~\ref{fig:teaser}. Hardware specifications of each platform are listed in Table~\ref{tab:robot models}. Despite large variations in hardware design, physical properties, and kinematic topology, \our transfers reliably to all platforms and achieves a 100\% task success rate. These results are consistent with simulation.

Beyond basic locomotion, we further evaluate our method in whole-body control scenarios. Our generalist policy on all robot platforms can robustly and accurately follow full-body commands issued from real-time teleoperation~\cite{Cheng2024OpenTeleVisionTW}, producing coordinated whole-body behaviors across diverse embodiments, as shown in Fig.~\ref{fig:teaser}. We also test the policy on several long-horizon loco-manipulation tasks, including plush toy picking, door opening, and traversal. These tasks require precise arm control and efficient full-body posture coordination. Despite the challenges, the policy achieves near-perfect success rates across all tasks.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/real_world.png}
    \caption{\small \textbf{Loco-manipulation sequence: plush toy picking, door opening, and traversal.}  
    The robot first walks toward the box on the right and bends to grasp the plush toy. Next, it opens the door with the other hand, walks through, stops in front of the box, squats, and places the toy inside.
    }
    \label{fig:real_world_experiment}
    \vspace{-10pt}
\end{figure}